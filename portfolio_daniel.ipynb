{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Concatenate, Dropout, Subtract, \\\n",
    "                        Flatten, MaxPooling2D, Multiply, Lambda, Add, Dot\n",
    "from tensorflow.keras.backend import constant\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "#from keras.engine.topology import Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import random_correlation\n",
    "from sklearn.datasets import make_sparse_spd_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, Concatenate, Dropout, Subtract, \\\n",
    "                        Flatten, MaxPooling2D, Multiply, Lambda, Add, Dot\n",
    "from keras.backend import constant\n",
    "from keras import optimizers\n",
    "\n",
    "#from keras.engine.topology import Layer\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras import initializers\n",
    "from keras.constraints import max_norm\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import random_correlation\n",
    "from sklearn.datasets import make_sparse_spd_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.08416849 -0.12172808]\n",
      " [-0.08416849  1.         -0.14799854]\n",
      " [-0.12172808 -0.14799854  1.        ]]\n",
      "[[0.1 0.1 0.1]]\n"
     ]
    }
   ],
   "source": [
    "## Generalisation to multiple stocks -- SET D= # of stocks\n",
    "N=30 # time disrectization\n",
    "m = 3#number of stocks\n",
    "S0 = np.random.normal(1,0.0,m).reshape(1,m) # initial value of the asset(s)\n",
    "X0=1  # initial wealth\n",
    "T=1 # maturity\n",
    "sigma=0.2 # volatility in Black Scholes\n",
    "sigma = np.abs(np.random.normal(0.2,0.0,m).reshape(m,))\n",
    "mu=np.random.normal(0.1,0.0,m).reshape(1,m)\n",
    "#mu = np.array([0.1,0.3])\n",
    "r=0.05\n",
    "gamma=0.0\n",
    "R=10**5 # number of Trajectories\n",
    "eigenvals = []\n",
    "if m > 1:\n",
    "    for _ in range(m):\n",
    "        eigenvals.append(np.random.uniform(0,m))\n",
    "    eigenvals = m/sum(eigenvals)*np.array(eigenvals)\n",
    "    rho = random_correlation.rvs(eigenvals)\n",
    "print(rho)\n",
    "print(mu)\n",
    "#rho = [[1,0],[0,1]]\n",
    "# logS= np.zeros((N,R))\n",
    "# logS[0,]=np.log(S0)*np.ones((1,R))\n",
    "\n",
    "# for i in range(R):\n",
    "#     for j in range(N-1):\n",
    "#         increment = np.random.normal(mu*T/N-(sigma)**2*T/(2*N),sigma*np.sqrt(T)/np.sqrt(N))\n",
    "#         logS[j+1,i] =logS[j,i]+increment\n",
    "\n",
    "# S=np.exp(logS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of neural networks for trading strategies\n",
    "\n",
    "#m = D # dimension of price\n",
    "d = 3 # number of layers in strategy\n",
    "n = 32  # nodes in the first but last layers\n",
    "\n",
    "# architecture is the same for all networks\n",
    "layers = []\n",
    "for j in range(N):\n",
    "    for i in range(d):\n",
    "        if i < d-1:\n",
    "            nodes = n\n",
    "            layer = Dense(nodes, activation='tanh',trainable=True,\n",
    "                      kernel_initializer=initializers.RandomNormal(0,0.5),#kernel_initializer='random_normal',\n",
    "                      bias_initializer=initializers.RandomNormal(0,0.5),\n",
    "                      name=str(i)+str(j))\n",
    "        else:\n",
    "            nodes = m\n",
    "            layer = Dense(nodes, activation='linear', trainable=True,\n",
    "                          kernel_initializer=initializers.RandomNormal(0,0.5),#kernel_initializer='random_normal',\n",
    "                          bias_initializer=initializers.RandomNormal(0,0.5),\n",
    "                          name=str(i)+str(j))\n",
    "        layers = layers + [layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the outcoming of trading via neural networks\n",
    "# Inputs is the training set below, containing the price S0, \n",
    "# again we record the trading strategies on separate input variables 'tradeeval' to read them out easily later\n",
    "price = Input(shape=(m,))\n",
    "trade = Input(shape=(m,))\n",
    "tradeeval = Input(shape=(m,))\n",
    "wealth = Input(shape=(1,))\n",
    "inputs = [price]+[trade]+[tradeeval]+[wealth]\n",
    "outputhelper=[]\n",
    "\n",
    "for j in range(N):\n",
    "    strategy = price\n",
    "    strategyeval=tradeeval\n",
    "    for k in range(d):\n",
    "        strategy= layers[k+(j)*d](strategy) # strategy at j is the alpha at j \n",
    "        strategyeval=layers[k+(j)*d](strategyeval)\n",
    "        \n",
    "    #implement strategy dependent interest\n",
    "\n",
    "    helper0a = Lambda(lambda x : K.sum(x,axis=1,keepdims=True))(strategy) #sum over all alphas\n",
    "    helper0b = Lambda(lambda x: K.less(1.0,x))(helper0a) #check if the sum of alphas is larger than 1\n",
    "    helper0b = Lambda(lambda x: tf.cast(x,tf.float32))(helper0b)\n",
    "    r_t = Lambda(lambda x: (mu.mean()-r)*(helper0a-1)*x+r)(helper0b) #adapt interest rate\n",
    "    #NOTE: set x to zero here if you want to turn off ^\n",
    "    \n",
    "    #print(r_t.shape)\n",
    "    \n",
    "    incr = Input(shape=(m,))\n",
    "    logprice= Lambda(lambda x : K.log(x))(price)\n",
    "    logprice = Add()([logprice, incr])\n",
    "    pricenew=Lambda(lambda x : K.exp(x))(logprice)\n",
    "    price=pricenew\n",
    "    logwealth= Lambda(lambda x : K.log(x))(wealth)    \n",
    "   \n",
    "    #logwealth= Lambda(lambda x : x+r*T/N)(logwealth) # <---- r value\n",
    "    helperR = Lambda(lambda x : x*T/N)(r_t)\n",
    "    logwealth = Add()([helperR,logwealth])\n",
    "  \n",
    "    helper1 = Multiply()([strategy, incr])\n",
    "    helper1 = Lambda(lambda x : K.sum(x,axis=1))(helper1)\n",
    "    logwealth = Add()([logwealth, helper1])\n",
    "    if m == 1:\n",
    "        helper2 = Multiply()([strategy, strategy])   \n",
    "        #helper2 = Lambda(lambda x : K.sum(x,axis=1))(helper2)\n",
    "        helper3 = Lambda(lambda x : x*sigma**2/2*T/N)(helper2)\n",
    "        helper3 = Lambda(lambda x: K.sum(x,axis=1))(helper3)\n",
    "        logwealth = Subtract()([logwealth, helper3])\n",
    "       \n",
    "    helper4 = Lambda(lambda x: K.sum(x,axis=1))(strategy)\n",
    "    #helper4 = Lambda(lambda x : x*r*T/N)(helper4) # <----- r value\n",
    "    helper4 = Multiply()([helper4,helperR])\n",
    "    \n",
    "    logwealth = Subtract()([logwealth, helper4])\n",
    "       \n",
    "\n",
    "    sigma_strat = Lambda(lambda x: sigma*x)(strategy) #Multiply the alphas by the sigmas\n",
    "    rho_temp =K.constant(rho,dtype='float32') #Cast correlation matrix into layer\n",
    "    #helper5 = Lambda(lambda x: tf.einsum('ki,ij,jk->k',x[0],x[1],x[2]))([sigma_strat,rho_temp,K.transpose(sigma_strat)])\n",
    "    if m>1 :\n",
    "        helper5 = Lambda(lambda x: K.dot(x,rho_temp))(sigma_strat)\n",
    "        helper5 = Lambda(lambda x: K.dot(x,K.transpose(sigma_strat)))(helper5) #Compute the matrix term\n",
    "        helper5 = Lambda(lambda x: tf.linalg.diag_part(x))(helper5)\n",
    "        helper5 = Lambda(lambda x : 0.5*x*T/N)(helper5)\n",
    "\n",
    "        logwealth = Subtract()([logwealth, helper5])\n",
    "    wealthnew=Lambda(lambda x : K.exp(x))(logwealth)# creating the wealth at time j+1\n",
    "    inputs = inputs + [incr]\n",
    "    outputhelper = outputhelper + [r_t] +[strategyeval] # here we collect the strategies    \n",
    "    wealth=wealthnew\n",
    "    #print(K.int_shape(logwealth))\n",
    "outputs = wealth\n",
    "#randomendowment = Lambda(lambda x : -0.0*(K.abs(x-1.0)+x-1.0))(price) \n",
    "#outputs = Add()([wealth,randomendowment])\n",
    "outputs = [outputs] + outputhelper\n",
    "#print(K.int_shape(outputhelper[0]))\n",
    "outputs = Concatenate()(outputs)\n",
    "\n",
    "model_MertonD = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ktrain = 10**5\n",
    "initialprice = S0\n",
    "initialwealth = X0\n",
    "\n",
    "#Generate correlated price processes\n",
    "\n",
    "from scipy.linalg import cholesky\n",
    "\n",
    "uncorr = [np.random.normal(0,np.sqrt(T)/np.sqrt(N),(Ktrain,m)) for i in range(N)] #generate uncorrelated price processes\n",
    "\n",
    "if m >1:\n",
    "    corr = []\n",
    "    U= cholesky(rho)\n",
    "    #U = np.array([[1,-1],[0,0]])\n",
    "    for unc in uncorr:\n",
    "        corr.append(unc@U*sigma+mu*T/N) #Correlate the processes with the Cholesky decomposition of rho\n",
    "else:\n",
    "    corr = [np.random.normal(mu*T/N,sigma*np.sqrt(T)/np.sqrt(N),(Ktrain,m)) for i in range(N)]\n",
    "# xtrain consists of the price S0, \n",
    "#the initial hedging being 0, and dummy variables hedgeeval where the strategies are evaluated, \n",
    "#the initial wealth and the increments of the log price process     \n",
    "\n",
    "xtrain = ([np.ones((Ktrain,1))@initialprice] +\n",
    "          [np.zeros((Ktrain,m))]+\n",
    "          [1*np.ones((Ktrain,m))] +\n",
    "          [initialwealth*np.ones((Ktrain,1))] + corr)\n",
    "          #[np.random.normal(mu*T/N,sigma*np.sqrt(T)/np.sqrt(N),(Ktrain,m)) for i in range(N)])\n",
    "\n",
    "ytrain=np.zeros((Ktrain,1+N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.89646076 14.25283769 14.70749452]]\n"
     ]
    }
   ],
   "source": [
    "#Mean-Variance optimisation\n",
    "\n",
    "cov = np.diag(sigma)@rho@np.diag(sigma)\n",
    "r_vec = r*np.ones(m)\n",
    "p = 0.052 #Required return - risk factor\n",
    "premium = (mu-r_vec).reshape(m,)\n",
    "lambd = (p-r)/np.einsum('i,ij,j',premium,cov,premium)\n",
    "w_opt = np.array(lambd*np.matrix(cov)**(-1)@premium)\n",
    "print(w_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.58398820985737\n",
      "524.816478226392\n"
     ]
    }
   ],
   "source": [
    "#Compute final wealth:\n",
    "wealth = initialwealth*np.ones((Ktrain,1))\n",
    "s_t = np.ones((Ktrain,1))@initialprice\n",
    "logS = np.log(s_t)\n",
    "logX = np.log(np.ones([Ktrain,1]))\n",
    "for incr in corr:\n",
    "    logS += incr\n",
    "    logX += incr@w_opt.T + (1-w_opt.sum())*r*T/N - 0.5*w_opt@cov@w_opt.T*T/N\n",
    "#print(np.exp(logX))\n",
    "print(np.exp(logX).mean())\n",
    "print(np.exp(logX).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true,y_pred):\n",
    "    z = - K.log(y_pred[:,0])#-((y_pred[:,0]**gamma-1)/gamma\n",
    "    z=K.mean(z)\n",
    "    return z\n",
    "#def custom_loss(y_true,y_pred):\n",
    "#    z = K.exp(- y_pred[:,0]*ra)#\n",
    "#    z=K.mean(z)\n",
    "#    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "adam=optimizers.Adam(lr=0.01)\n",
    "\n",
    "model_MertonD.compile(optimizer='adam',loss=custom_loss)#,experimental_run_tf_function=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 34s 344us/step - loss: -0.1560\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    model_MertonD.fit(x=xtrain,y=ytrain, epochs=1,verbose=True,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1681487\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_MertonD.predict(xtrain)\n",
    "print(np.mean(-np.log(y_pred[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1005818 1.0235593 1.5132884 ... 1.2700514 1.0159364 1.1788353]\n",
      "[0.05030418 0.05030418 0.05030418 ... 0.05030418 0.05030418 0.05030418]\n",
      "[0.27702814 0.27702814 0.27702814 ... 0.27702814 0.27702802 0.27702802]\n",
      "[1.1017742 1.1017742 1.1017742 ... 1.1017742 1.1017742 1.1017742]\n"
     ]
    }
   ],
   "source": [
    "for i in range(m+2):\n",
    "    print(y_pred[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 91)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median wealth for portfolio with 3 assets: 1.348204\n",
      "STD of wealth for portfolio with 3 assets: 0.74009866\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_pred[:,0],bins = 25,range=(0,10))\n",
    "plt.xlabel('Final wealth')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Portfolio with ' +str(m) + ' assets')\n",
    "plt.show()\n",
    "print('Mean wealth for portfolio with',m,'assets:',np.mean(y_pred[:,0]))\n",
    "print('STD of wealth for portfolio with',m,'assets:',np.std(y_pred[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    plt.plot(S[:,i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X[N-1,:])\n",
    "plt.show()\n",
    "print(np.mean(X[N-1,:]))\n",
    "print(np.std(X[N-1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=10#Choose a number between 1 and N-1\n",
    "Ktest=60\n",
    "xtest = ([initialprice*np.ones((Ktest,m))] +\n",
    "          [np.zeros((Ktest,m))]+\n",
    "          [np.linspace(0.7,1.5,Ktest)] +#change this if you go to higher dimensions\n",
    "          [initialwealth*np.ones((Ktest,m))]+\n",
    "          [np.random.normal(mu*T/N,sigma*np.sqrt(T)/np.sqrt(N),(Ktest,m)) for i in range(N)])\n",
    "\n",
    "\n",
    "#Comparison of learned and true alpha\n",
    "s=np.linspace(0.7,1.5,Ktest)\n",
    "\n",
    "for k in range(1,N):\n",
    "    truestrat=(mu-r)/(sigma**2*(1-gamma))*np.ones(Ktest)\n",
    "    learnedstrat=model_Merton.predict(xtest)[:,k]\n",
    "    plt.plot(s,learnedstrat,s,truestrat)\n",
    "plt.show()\n",
    "print((mu-r)/(sigma**2*(1-gamma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
